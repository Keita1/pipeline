{
  "metadata" : {
    "name" : "Ratings",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/tmp/repo",
    "customRepos" : null,
    "customDeps" : [ "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M1", "org.elasticsearch:elasticsearch-spark_2.10:2.1.0", "org.apache.spark:spark-streaming-kafka-assembly_2.10:1.4.0", "org.apache.spark % spark-graphx_2.10 % 1.4.1", "- org.apache.spark % spark-core_2.10 % _", "- org.apache.spark % spark-streaming_2.10 % _", "- org.apache.hadoop % _ % _" ],
    "customImports" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "127.0.0.1"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Creating the context to use `DataFrame`"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Load the rating dataset (cold data)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val ratingsDF = sqlContext.read.format(\"json\").load(\"file:/root/pipeline/datasets/dating/ratings.json.bz2\")\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : ":markdown\nThere are ${ratingsDF.count} ratings",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Let's chek the leading ones"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "widgets.TableChart(ratingsDF, maxPoints=25)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### It's a graph"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We can create structures representing the ratings as `Edge`s and the raters as `Node`s."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val edges = ratingsDF.rdd.zipWithIndex.map(r => Edge(r._2, (r._1.getLong(0), r._1.getLong(2)), r._1.getLong(1)))\nval nodes = edges.flatMap(e => List(e.ends._1, e.ends._2)).distinct.map(x => Node(x, \"\"))\n()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "What about plotting sample of that graph"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val gs = new java.io.Serializable {\n  val ed = edges.sample(false, 0.00001).collect.toList\n  val nd = ed.flatMap(e => List(e.ends._1, e.ends._2)).distinct.map(x => Node(x, \"\"))\n  @transient val allG = ed :::nd\n  @transient val plot = widgets.GraphChart(allG, maxPoints=allG.size)\n}.plot",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Quick look at the distribution of the number of ratings"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### First independently of the gender"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The following counts the ratings per rater."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val ft = ratingsDF.groupBy(\"fromUserId\").agg(count(\"toUserId\").as(\"cnt\")).orderBy($\"cnt\".desc)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : ":markdown\nThere are actually ${ft.count} raters",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Quick glance at the distribution of the rating freaks"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "widgets.LineChart(ft.select(\"cnt\").rdd.map(_.getLong(0)).take(100), fields=Some((\"X\", \"Y\")), maxPoints=100)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### What about the distribution within genders"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### First we load the cold data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val genderDF = sqlContext.read.format(\"json\").load(\"file:/root/pipeline/datasets/dating/gender.json.bz2\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Looking at number of ratings per gender (joining)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val withGender = ft.join(genderDF, ft(\"fromUserId\") === genderDF(\"id\"))\nval fems = withGender.filter($\"gender\" === \"F\").orderBy($\"cnt\".desc).select(\"cnt\").rdd.map(_.getLong(0)).take(100)\nval mals = withGender.filter($\"gender\" === \"M\").orderBy($\"cnt\".desc).select(\"cnt\").rdd.map(_.getLong(0)).take(100)\nval others = withGender.filter($\"gender\" === \"U\").orderBy($\"cnt\".desc).select(\"cnt\").rdd.map(_.getLong(0)).take(100)\n\nwidgets.LineChart(fems, fields=Some((\"X\", \"Y\")), maxPoints=100) ++ \n  widgets.LineChart(mals, fields=Some((\"X\", \"Y\")), maxPoints=100) ++ \n  widgets.LineChart(others, fields=Some((\"X\", \"Y\")), maxPoints=100)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### It's definitely a Graph(X)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Hence we can create a spark Graph instance using the ratings and gender datasets."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.graphx.{Edge=>GEdge, _}\nval gnodes = nodes.map(_.id).toDF\nval xVertices = gnodes.join(genderDF, gnodes(\"_1\") === genderDF(\"id\")).select(gnodes(\"_1\"), genderDF(\"gender\")).map(r => (r.getLong(0), r.getString(1)))\nval xEdges = edges.map(e => org.apache.spark.graphx.Edge(e.ends._1, e.ends._2, e.value))\nval graph = Graph(xVertices, xEdges)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### How are those raters performing with their buddies: PageRank"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val pg = graph.ops.pageRank(0.001)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "What does that look like?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "pg.vertices.take(10)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The schema might be interesting to check before hacking that one."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val pgv = pg.vertices.toDF\npgv.schema",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Check the PageRank distributions within genders"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val bestG = pgv.join(genderDF, pgv(\"_1\") === genderDF(\"id\"))\n\nimport notebook.front.third.wisp._\nval bf = bestG.filter($\"gender\" === \"F\").orderBy($\"_2\".desc).select($\"_2\").map(_.getDouble(0)).zipWithIndex.map(_.swap).take(1000).toList\nval bm = bestG.filter($\"gender\" === \"M\").orderBy($\"_2\".desc).select($\"_2\").map(_.getDouble(0)).zipWithIndex.map(_.swap).take(1000).toList\nval bu = bestG.filter($\"gender\" === \"U\").orderBy($\"_2\".desc).select($\"_2\").map(_.getDouble(0)).zipWithIndex.map(_.swap).take(1000).toList\nPlot(Seq(Pairs(bf, \"line\"), Pairs(bm, \"line\"), Pairs(bu, \"line\")))",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Streaming: Kafka"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Creating the domain object"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "case class Rating(fromUserId: Int, toUserId: Int, rating: Int) {\n  def toCSV=s\"$fromUserId,$rating,$toUserId\"\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Kafka Producer"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import java.util.Properties\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.Future\n\nimport kafka.producer.{KeyedMessage, Producer, ProducerConfig}\n\nval props = new Properties()\nprops.put(\"metadata.broker.list\", \"localhost:9092,localhost:9093\")\nprops.put(\"serializer.class\", \"kafka.serializer.StringEncoder\")\nval producerConfig = new ProducerConfig(props)\nval producer = new Producer[String, String](producerConfig)\n\n// Guard to stop the producer\nvar stopSending = false\n\n// future that issues a future.\n// a future sends a message after having waited for up to 500ms\ndef sendMsg:Unit = Future {\n  Thread.sleep((scala.util.Random.nextDouble * 500).toLong)\n  producer.send {\n    val msg = Rating(scala.util.Random.nextInt(10000), scala.util.Random.nextInt(10), scala.util.Random.nextInt(10000))\n    new KeyedMessage[String, String](\"ratings\", msg.fromUserId.toString, msg.toCSV)\n  }\n  if (!stopSending) sendMsg\n}\nsendMsg",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Viz"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Reactive list of data (capped at 20 elements)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val list = ul(20)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Reactive line plot"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val line = widgets.LineChart[Seq[(Long, Double)]](Seq.empty[(Long, Double)], fields=Some((\"X\", \"Y\")), maxPoints=100)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Spark Streaming"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The context and kafka conf"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nval ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"ratings\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Biz:\n* Consuming Kafka, \n* creating Ratings, \n* computing moving average\n* update list and line plot"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\n// connect (direct) to kafka\nval ratingsStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n  ssc, kafkaParams, topics)\n\n// transform the CSV Strings into Ratings\nval msgs = ratingsStream.transform {\n  (message: RDD[(String, String)], batchTime: Time) => {\n    // convert each RDD from the batch into a DataFrame\n    val df = message.map(_._2.split(\",\"))\n                    .map(rating => \n                         Rating(rating(0).trim.toInt, rating(1).trim.toInt, rating(2).trim.toInt)\n                    ).toDF(\"fromuserid\", \"touserid\", \"rating\")\n\n    // add the batch time to the DataFrame\n    val dfWithBatchTime = df.withColumn(\"batch_time\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n    dfWithBatchTime.rdd\n  }\n}\n\n//show the first ten of each RDD\nmsgs.foreachRDD(rdd => list.appendAll(rdd.take(10).toList.map(_.toString)))\n\n// show the moving average of 10s in the plot\nval data = new collection.mutable.ArrayBuffer[(Long, Double)]()\nval w = msgs.window(Seconds(10), Seconds(2)).foreachRDD{(rdd, t) => \n              val ndata = Seq((t.milliseconds, rdd.map(_.getAs[Int](\"rating\")).mean.toDouble))\n              org.apache.log4j.Logger.getLogger(\"lines\").info(\"# \" + ndata.size)\n              line.addAndApply(data ++= ndata)\n             }\n",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Start consuming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Stop all"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the streaming context (keeping the spark context up, just in case)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "ssc.stop(false)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the producer"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "stopSending = true",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}