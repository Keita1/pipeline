{
  "metadata" : {
    "name" : "Ratings from Cassandra",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M2", "org.elasticsearch:elasticsearch-spark_2.10:2.1.0", "org.apache.spark:spark-streaming-kafka-assembly_2.10:1.4.0", "org.apache.spark % spark-graphx_2.10 % 1.4.1", "- org.apache.spark % spark-core_2.10 % _", "- org.apache.spark % spark-streaming_2.10 % _", "- org.apache.hadoop % _ % _" ],
    "customImports" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "127.0.0.1",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.executor.cores" : "1",
      "spark.executor.memory" : "512m",
      "spark.cores.max" : "1",
      "spark.eventLog.enabled" : "true",
      "spark.eventLog.dir" : "logs/spark",
      "spark.scheduler.mode" : "FAIR"
    }
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql._\nval sqlContext = new SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "case class Rating(fromUserId: Int, toUserId: Int, rating: Int) {\n  def toCSV=s\"$fromUserId,$rating,$toUserId\"\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import java.util.Properties\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.Future\n\nimport kafka.producer.{KeyedMessage, Producer, ProducerConfig}\n\nval props = new Properties()\nprops.put(\"metadata.broker.list\", \"localhost:9092,localhost:9093\")\nprops.put(\"serializer.class\", \"kafka.serializer.StringEncoder\")\nval producerConfig = new ProducerConfig(props)\nval producer = new Producer[String, String](producerConfig)\n\n// Guard to stop the producer\nvar stopSending = false\n\n// future that issues a future.\n// a future sends a message after having waited for up to 500ms\ndef sendMsg:Unit = Future {\n  Thread.sleep((scala.util.Random.nextDouble * 500).toLong)\n  producer.send {\n    val msg = Rating(scala.util.Random.nextInt(10000), scala.util.Random.nextInt(10), scala.util.Random.nextInt(10000))\n    new KeyedMessage[String, String](\"ratings\", msg.fromUserId.toString, msg.toCSV)\n  }\n  if (!stopSending) sendMsg\n}\nsendMsg",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nval ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"ratings\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\n// connect (direct) to kafka\nval ratingsStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n  ssc, kafkaParams, topics)\n\n// transform the CSV Strings into Ratings\nval msgs = ratingsStream.foreachRDD { (message: RDD[(String, String)], batchTime: Time) => \n  // convert each RDD from the batch into a DataFrame\n  val df = message.map(_._2.split(\",\"))\n                  .map(rating => \n                       Rating(rating(0).trim.toInt, rating(1).trim.toInt, rating(2).trim.toInt)\n                  ).toDF(\"fromuserid\", \"touserid\", \"rating\")\n\n  // add the batch time to the DataFrame\n  val dfWithBatchTime = df.withColumn(\"batchtime\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n\n  // save the DataFrame to Cassandra\n  // Note:  Cassandra has been initialized through spark-env.sh\n  //        Specifically, export SPARK_JAVA_OPTS=-Dspark.cassandra.connection.host=127.0.0.1\n  dfWithBatchTime.write.format(\"org.apache.spark.sql.cassandra\")\n    .mode(SaveMode.Append)\n    .options(Map(\"keyspace\" -> \"pipeline\", \"table\" -> \"real_time_ratings\"))\n    .save()\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val genderDF = sqlContext.read.format(\"json\").load(\"file:/root/pipeline/datasets/dating/gender.json.bz2\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def scopeBox(gender:String, column:String) = new java.io.Serializable {\n  @transient val sqc = sqlContext\n  \n  val ratingsC = sqc\n                  .read\n                  .format(\"org.apache.spark.sql.cassandra\")\n                  .options(Map( \"table\" -> \"real_time_ratings\", \"keyspace\" -> \"pipeline\"))\n                  .load()// This DataFrame will use a spark.cassandra.input.size of 5000\n\n  val ratingColumn = column\n  \n  val sumRatings = ratingsC.groupBy($\"touserid\").agg(Map(ratingColumn -> \"sum\")).withColumnRenamed(\"touserid\", \"id\")\n\n  val males = sumRatings.join(genderDF, sumRatings(\"id\") === genderDF(\"id\"))\n                          .filter($\"gender\" === gender)\n                          .orderBy(sumRatings(\"SUM(\"+ratingColumn+\")\").asc)\n                          .select(sumRatings(\"SUM(\"+ratingColumn+\")\"))\n                          .map(row => (row.getLong(0)))\n                          .zipWithIndex\n\n  val count = males.count\n  val min = males.min._1\n  val max = males.max._1\n  val qs = males.filter(x => \n                        x._2 == (count * 0.25).toInt || \n                        x._2 == (count * 0.50).toInt || \n                        x._2 == (count * 0.75).toInt\n                       ).collect().toList\n\n\n  val (q1:Long, _)::(med:Long, _)::(q3:Long, _)::_ = qs\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import notebook.front.third.wisp._\nimport com.quantifind.charts.highcharts.Highchart._\nimport com.quantifind.charts.highcharts._\n\nval column = \"batchtime\" // crap thingy here, cassandra seems to mess at write time rating and batchtime\n\nval f =  { \n val b = scopeBox(\"F\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval m =  { \n val b = scopeBox(\"M\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval u =  { \n val b = scopeBox(\"U\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval series =  Series(\n               Seq(f, m, u), \n               chart=Some(SeriesType.boxplot)\n             )\nval plot = PlotH(Highchart(series, \n                           title=Some(Title(\"Boxplots of rating distributions within Gender\")),\n                           xAxis=Some(Array(Axis(categories=Some(Array(\"Female\", \"Male\", \"Unknown\")))))\n                          )\n                )",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}
